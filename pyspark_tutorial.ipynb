{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Tutorial').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following variables are required to use FileSystem to scan directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "URI           = spark.sparkContext._gateway.jvm.java.net.URI\n",
    "Path          = spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.Path\n",
    "FileSystem    = spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.FileSystem\n",
    "Configuration = spark.sparkContext._gateway.jvm.org.apache.hadoop.conf.Configuration\n",
    "FileStatus    = spark.sparkContext._gateway.jvm.org.apache.hadoop.fs.FileStatus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properties for comma delimited files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = {\n",
    "    \"header\": \"true\",\n",
    "    \"delimiter\": \",\",\n",
    "    \"inferSchema\": \"false\",\n",
    "    \"ignoreLeadingWhiteSpace\": \"true\",\n",
    "    \"ignoreTrailingWhiteSpace\": \"true\",\n",
    "    \"quote\": \"\\\"\",\n",
    "    \"escape\": \"\\\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function: readFile\n",
    "3 input variables: path, properties and boolean if you want to create a source_path column\n",
    "Output: DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(dataFilePath, fileProperties, source_path):\n",
    "    inputDF = spark.read.format(\"csv\")\\\n",
    "        .option(\"quote\", \"\")\\\n",
    "        .option(\"header\", fileProperties.get(\"header\"))\\\n",
    "        .option(\"delimiter\", fileProperties.get(\"delimiter\"))\\\n",
    "        .option(\"inferSchema\", fileProperties.get(\"inferSchema\"))\\\n",
    "        .option(\"ignoreLeadingWhiteSpace\", fileProperties.get(\"ignoreLeadingWhiteSpace\"))\\\n",
    "        .option(\"ignoreTrailingWhiteSpace\", fileProperties.get(\"ignoreTrailingWhiteSpace\"))\\\n",
    "        .option(\"quote\", fileProperties.get(\"quote\"))\\\n",
    "        .option(\"escape\", fileProperties.get(\"escape\"))\\\n",
    "        .load(dataFilePath)\n",
    "        \n",
    "    if (source_path):\n",
    "        inputDF = inputDF.withColumn(\"source_path\", input_file_name())\n",
    "    else:\n",
    "        inputDF\n",
    "    return inputDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function: readMultipleCSV\n",
    "4 input variables: path, table name, properties and boolean if you want to create a source_path column\n",
    "Output: DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readMultipleCSV(dataPath, table, fileProperties, source_path):\n",
    "    inputs = list()\n",
    "    \n",
    "    fs = FileSystem.get(URI(\"localhost\"), Configuration())\n",
    "    status = fs.listStatus(Path(dataPath))\n",
    "    for fileStatus in status:\n",
    "        path = fileStatus.getPath().toString()\n",
    "        inputs.append(path)\n",
    "        \n",
    "    input = filter(lambda x: table in x, inputs)\n",
    "    \n",
    "    for idx,f in enumerate(input):\n",
    "        if idx == 0:\n",
    "            inputDF = readFile(f, fileProperties, source_path)\n",
    "            outputDF = inputDF\n",
    "        else:\n",
    "            inputDF = readFile(f, fileProperties, source_path)\n",
    "            outputDF=outputDF.unionAll(inputDF)\n",
    "        \n",
    "    return outputDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read nonEmp data: source_path = True as we want to pull the date from file name; rename ST and CTY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonEmp = readMultipleCSV(\"de-tutorial/src/main/resources/data/\", \"nonemp\", properties, True)\\\n",
    "    .withColumn(\"YEAR\", concat(lit(20), regexp_extract(col(\"source_path\"), \"([0-9]{2})\", 1)))\\\n",
    "    .withColumnRenamed(\"ST\", \"STATE\")\\\n",
    "    .withColumnRenamed(\"CTY\", \"COUNTY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add new properties for population table as it is '|' delimited opposed to ',' delimited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "popProperties = {\n",
    "    \"header\": \"true\",\n",
    "    \"delimiter\": \"|\",\n",
    "    \"inferSchema\": \"false\",\n",
    "    \"ignoreLeadingWhiteSpace\": \"true\",\n",
    "    \"ignoreTrailingWhiteSpace\": \"true\",\n",
    "    \"quote\": \"\\\"\",\n",
    "    \"escape\": \"\\\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to unpivot/melt population DataFrame\n",
    "\n",
    "      Current:\n",
    "       STATE|POP_2018|POP_2017\n",
    "           x|      20|      22\n",
    "      \n",
    "      We want:\n",
    "       STATE|YEAR|POP\n",
    "           x|2018| 20\n",
    "           x|2017| 22\n",
    "           \n",
    "Function: unpivot\n",
    "4 input variables: DataFrame, by fields, key column name, val column name\n",
    "Output: DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpivot(df, by, key, val):\n",
    "\n",
    "    # Filter dtypes and split into column names and type description\n",
    "    cols, dtypes = zip(*((c, t) for (c, t) in df.dtypes if c not in by))\n",
    "    # Spark SQL supports only homogeneous columns\n",
    "    assert len(set(dtypes)) == 1, \"All columns have to be of the same type\"\n",
    "\n",
    "    # Create and explode an array of (column_name, column_value) structs\n",
    "    kvs = explode(array([\n",
    "      struct(lit(c).alias(key), col(c).alias(val)) for c in cols\n",
    "    ])).alias(\"kvs\")\n",
    "\n",
    "    return df.select(by + [kvs]).select(by + [\"kvs.\"+key, \"kvs.\"+val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read population statistic data using new properties and source_path = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = readFile(\"de-tutorial/src/main/resources/data/sub-est2018_all.csv\", popProperties, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpivot population statistics using function. Drop unwanted field before unpivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPop = unpivot(pop.drop('SUMLEV', 'PLACE', 'COUSUB', 'CONCIT', 'PRIMGEO_FLAG', 'FUNCSTAT', 'CENSUS2010POP', 'ESTIMATESBASE2010'), ['STATE', 'COUNTY', 'NAME'], 'YEAR', 'POPULATION')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read states mapping table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = readFile(\"de-tutorial/src/main/resources/data/states.csv\", properties, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join states mapping table onto unpivoted population statistics. Extract year from 'YEAR' column using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "enrichPop = newPop.join(broadcast(states), ['STATE'], 'left_outer')\\\n",
    "    .withColumn(\"YEAR\", regexp_extract(col('YEAR'), \"([0-9]{4})\", 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join enriched Population DataFrame onto the NonEmployer statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDF = nonEmp.join(enrichPop, ['STATE', 'COUNTY', 'YEAR'], \"left_outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save output to parquet, partitioned by \"STNAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDF.write\\\n",
    "    .mode(\"Overwrite\")\\\n",
    "    .partitionBy(\"STNAME\")\\\n",
    "    .parquet(\"data.parq\")\\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
